# RAG実装詳細設計

## 目的

本ドキュメントでは、RAG（Retrieval-Augmented Generation）システムの詳細設計について記載します。前回の調査ドキュメント（[001-RAG実装調査.md](001-RAG実装調査.md)）で検討した技術スタックを基に、具体的な実装方法と構成について詳細を定義します。

## システム構成図

```mermaid
graph TB
    subgraph "ローカル環境"
        A[ドキュメント] --> B[ドキュメントローダー]
        B --> C[テキスト分割]
        C --> D[埋め込み処理]
        D --> E[ベクトルDB\nDuckDB + VSS]
        F[クエリ] --> G[埋め込み処理]
        G --> H[ベクトル検索]
        E --> H
        H --> I[関連ドキュメント取得]
        I --> J[プロンプト生成]
        J --> K[回答生成]
    end
    
    subgraph "外部サービス"
        L[MCP] <--> M[API Gateway]
        M <--> N[RAG API]
    end
    
    N <--> G
    K --> N
```

## 技術コンポーネント詳細

### 1. LangChain.js

LangChain.jsは、フレームワークとしてRAGの各コンポーネントを統合するために使用します。

**バージョン情報**:
- langchain: ^0.2.0
- @langchain/openai: ^0.0.6（OpenAIスタイルのAPIインターフェース用）
- @langchain/community: ^0.0.20（ローダーと各種コネクタ用）

### 2. 埋め込みモデル

pfnet/plamo-embedding-1bは日本語対応の埋め込みモデルで、LM Studioを介してローカルで実行します。

**特性**:
- 埋め込みサイズ: 1024次元
- 言語サポート: 日本語と英語のバイリンガル対応
- コンテキスト長: 8192トークン
- ファイルサイズ: 約1.62GB

### 3. ベクトルデータベース

DuckDB + VSSをベクトルデータベースとして使用します。SQLベースで柔軟なクエリが可能であり、高速なベクトル検索をサポートします。

**設定情報**:
- 保存形式: ローカルファイルシステム
- 検索アルゴリズム: HNSW（Hierarchical Navigable Small World）

### 4. MCP連携

MCPとの連携にはRESTful APIを提供します。

**API仕様**:
- エンドポイント: `/api/query`
- メソッド: POST
- リクエストボディ:
  ```json
  {
    "query": "質問文",
    "project_context": "プロジェクト識別子",
    "max_results": 5
  }
  ```
- レスポンス:
  ```json
  {
    "answer": "生成された回答",
    "sources": [
      {
        "title": "参照ドキュメント名",
        "content": "参照内容",
        "relevance_score": 0.92
      }
    ]
  }
  ```

## 実装コード例

### 1. 基本的なプロジェクト構成

```
/mcp-rag/
├── src/
│   ├── config/
│   │   └── index.js       # 設定ファイル
│   ├── loaders/           # ドキュメントローダー
│   │   ├── markdown.js
│   │   ├── pdf.js
│   │   └── index.js
│   ├── embeddings/        # 埋め込み処理
│   │   └── plamo.js
│   ├── vectorstore/       # ベクトルストア
│   │   └── duckdb.js
│   ├── rag/               # RAG実装
│   │   ├── index.js
│   │   ├── processor.js
│   │   └── prompt.js
│   ├── api/               # API実装
│   │   ├── routes.js
│   │   └── server.js
│   └── index.js           # エントリーポイント
├── docs/                  # プロジェクトドキュメント
├── scripts/               # ユーティリティスクリプト
│   ├── index-docs.js      # ドキュメントインデックス作成
│   └── test-query.js      # クエリテスト
└── package.json
```

### 2. LM Studioによる埋め込みモデル設定

LM Studioを使用してplamo-embedding-1bモデルをローカルで実行し、OpenAI互換のAPIエンドポイントとして提供します。設定方法は以下の通りです：

1. LM Studioをダウンロードしてインストール
2. Settings > Local Serverタブで以下を設定:
   - Embedding Model: plamo-embedding-1b
   - 「Enable Embedding Model API」をオン
   - Server Port: 1234（デフォルト）
3. 「Start Server」をクリックしてサーバーを起動

### 3. ドキュメント処理コード例

以下は、Markdownドキュメントをロードし、チャンク分割して埋め込みを生成するコード例です：

```javascript
// src/loaders/markdown.js
const { DirectoryLoader } = require("langchain/document_loaders/fs/directory");
const { TextLoader } = require("langchain/document_loaders/fs/text");
const { RecursiveCharacterTextSplitter } = require("langchain/text_splitter");

// Markdownファイルを含むディレクトリからドキュメント読み込み
async function loadMarkdownDocuments(directoryPath) {
  const loader = new DirectoryLoader(
    directoryPath,
    {
      ".md": (path) => new TextLoader(path)
    }
  );
  
  const docs = await loader.load();
  console.log(`Loaded ${docs.length} markdown documents`);
  return docs;
}

// テキスト分割処理
async function splitDocuments(documents) {
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200,
  });
  
  const splitDocs = await textSplitter.splitDocuments(documents);
  console.log(`Split into ${splitDocs.length} chunks`);
  return splitDocs;
}

module.exports = {
  loadMarkdownDocuments,
  splitDocuments,
};
```

### 4. DuckDB + VSS連携

```javascript
// src/vectorstore/duckdb.js
const duckdb = require("duckdb");

async function setupVectorStore(dbPath) {
  const db = new duckdb.Database(dbPath);
  const conn = db.connect();

  // テーブル作成
  await conn.run(`
    CREATE TABLE IF NOT EXISTS documents (
      id VARCHAR PRIMARY KEY,
      content TEXT,
      metadata JSON,
      embedding FLOAT[]
    );
  `);

  console.log("DuckDB + VSS setup complete");
  return conn;
}

module.exports = {
  setupVectorStore,
};
```

## 実装における注意点

### 1. パフォーマンス最適化

- **バッチ処理**: 大量のドキュメントを処理する場合は、バッチ処理を実装して、メモリ使用量を抑える
- **モデル量子化**: plamo-embedding-1bモデルをより軽量化するために、量子化バージョンを検討
- **キャッシュ機構**: 頻繁に使用される埋め込みをキャッシュして計算コストを削減

### 2. エラーハンドリング

- **埋め込みモデルの異常**: LM Studioサーバーが停止している場合のフォールバック
- **ドキュメントパース失敗**: 様々な形式のドキュメント処理における例外処理
- **リトライ機構**: 一時的な障害から回復するためのリトライロジック

### 3. セキュリティ考慮事項

- **APIアクセス制限**: RAG APIへのアクセス制御
- **入力検証**: ユーザー入力のサニタイズによるインジェクション攻撃防止
- **機密情報フィルタリング**: 出力に機密情報が含まれないようにする仕組み

## 今後の検討事項

1. **インデックス更新メカニズム**: ドキュメントが更新された場合の差分インデックス更新
2. **複数言語対応**: 日本語以外の言語への対応
3. **評価メトリクス**: RAGシステムの性能を評価するためのメトリクス設計
4. **UI開発**: 開発者向けのRAG管理インターフェース

## 参考リソース

- [LangChain.js ドキュメント](https://js.langchain.com/docs/)
- [LM Studio - ローカルLLMおよび埋め込みモデルの実行](https://lmstudio.ai/)
- [DuckDB公式ドキュメント](https://duckdb.org/)
